<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>代码表征预训练语言模型学习指南：原理、分析和代码 | LinLi's Blog</title><meta name="author" content="Lin Li"><meta name="copyright" content="Lin Li"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="代码表征预训练语言模型学习指南：原理、分析和代码转载自 https:&#x2F;&#x2F;mp.weixin.qq.com&#x2F;s?__biz&#x3D;MzIwMTc4ODE0Mw&#x3D;&#x3D;&amp;mid&#x3D;2247582074&amp;idx&#x3D;2&amp;sn&#x3D;4dc8f39f1ab39c5275e23be845a65dc8&amp;chksm&#x3D;96eb5efaa19cd7ec1f351ad4232cfdd51893be4ef1c">
<meta property="og:type" content="article">
<meta property="og:title" content="代码表征预训练语言模型学习指南：原理、分析和代码">
<meta property="og:url" content="http://example.com/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/index.html">
<meta property="og:site_name" content="LinLi&#39;s Blog">
<meta property="og:description" content="代码表征预训练语言模型学习指南：原理、分析和代码转载自 https:&#x2F;&#x2F;mp.weixin.qq.com&#x2F;s?__biz&#x3D;MzIwMTc4ODE0Mw&#x3D;&#x3D;&amp;mid&#x3D;2247582074&amp;idx&#x3D;2&amp;sn&#x3D;4dc8f39f1ab39c5275e23be845a65dc8&amp;chksm&#x3D;96eb5efaa19cd7ec1f351ad4232cfdd51893be4ef1c">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2023-06-03T14:28:17.000Z">
<meta property="article:modified_time" content="2023-06-03T14:29:06.620Z">
<meta property="article:author" content="Lin Li">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '代码表征预训练语言模型学习指南：原理、分析和代码',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-06-03 22:29:06'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">63</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="LinLi's Blog"><span class="site-name">LinLi's Blog</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">代码表征预训练语言模型学习指南：原理、分析和代码</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-06-03T14:28:17.000Z" title="发表于 2023-06-03 22:28:17">2023-06-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-06-03T14:29:06.620Z" title="更新于 2023-06-03 22:29:06">2023-06-03</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="代码表征预训练语言模型学习指南：原理、分析和代码"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="代码表征预训练语言模型学习指南：原理、分析和代码"><a href="#代码表征预训练语言模型学习指南：原理、分析和代码" class="headerlink" title="代码表征预训练语言模型学习指南：原理、分析和代码"></a>代码表征预训练语言模型学习指南：原理、分析和代码</h1><p>转载自 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247582074&idx=2&sn=4dc8f39f1ab39c5275e23be845a65dc8&chksm=96eb5efaa19cd7ec1f351ad4232cfdd51893be4ef1c6358825c576351f7486ee0c4e33a5b317&scene=27">https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247582074&amp;idx=2&amp;sn=4dc8f39f1ab39c5275e23be845a65dc8&amp;chksm=96eb5efaa19cd7ec1f351ad4232cfdd51893be4ef1c6358825c576351f7486ee0c4e33a5b317&amp;scene=27</a>   </p>
<p>自从 2020 年 CodeBERT 开了代码表征预训练模型（本文称之为  CodePTM）这个新坑后，在短短两年的时间内出现了若干个程序设计语言（Programming Language，称之为 PL，与  Natural Language，也就是 NL 对应）语言模型。它们的共同特点是大部分用于处理 PL（或 Source  Code）时所采用的技术是从 NLP 中的 Transformer-based 模型迁移而来，但这些 CodePTMs  又各有特点，从训练的角度出发即不同的预训练&#x2F;微调策略，从数据的角度出发，比如是否使用了代码的结构信息，即抽象语法树（Abstract  Syntax Tree，本文将其简称为 AST）及其变体。</p>
<p>而从架构出发，这些 Code 预训练模型大致可以被分为以下这三类：</p>
<ol>
<li>encoder-only：CuBERT、CodeBERT、GraphCodeBERT 等。</li>
<li>decoder-only：CodeGPT、GPT-C 等。</li>
<li>encoder-decoder：PLBART、CodeT5 等。</li>
</ol>
<p><strong>本文对各个 CodePTM 建模编程语言的思想进行回顾，并简要分析了一下它们的特色。</strong>对文中提到的所有 CodePTMs  的描述主要从背景、预训练策略、微调策略以及下游任务这几个角度出发进行分析，考虑到这些模型之间都存在一些共性以及文章篇幅原因，文中略去了一些通用的处理手段和细节，因此对各部分的分析讲解详略不一，不过都保留了建模编程语言最核心的思想。阅读前需要对 Transformer 有一定的了解。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230530212850342.png" alt="image-20230530212850342"></p>
<h2 id="CuBERT"><a href="#CuBERT" class="headerlink" title="CuBERT"></a><strong>CuBERT</strong></h2><p>Learning and Evaluating Contextual Embedding of Source Code. ICML 2020.</p>
<p><em><a target="_blank" rel="noopener" href="https://aclanthology.org/2020.findings-emnlp.139/">https://aclanthology.org/2020.findings-emnlp.139/</a></em></p>
<p>CuBERT，即 Code Understanding BERT，和后面提到的 CodeBERT 可被归为同一个时期的工作，虽是首个提出 Code  预训练模型的工作，但和 CodeBERT 相比，其影响力较小（在写这篇文章的时候 CuBERT 引用还没过百），具体原因个人认为是它仅对  Python 语言进行建模（CodeBERT 同时对 6 种编程语言建模），且它的下游任务和 CodeBERT  相比不太丰富，主要是以代码理解任务为主。</p>
<p>作者通过 GitHub 采集了 7.4M Python 语言编写的程序用于 CuBERT 的预训练，将基于 Transformer  类模型处理自然语言的手段迁移到了编程语言上，使用的模型架构和训练方式直接照搬了 BERT-Large（24 层 16  个注意力头），然后使用了一个处理过的 ETH Py150 [1] 数据集进行微调。与此同时，作者还训练了一组 Word2Vec embeddings、Bi-LSTM 模型和 Transformer 模型用于比较。</p>
<p>就任务而言，作者构建了一个（在当时全新的）Benchmark，其中包含了：</p>
<ol>
<li><p>五个分类任务（具体细节和描述可参考原文附录） </p>
</li>
<li><ol>
<li>Variable-Misuse Classification</li>
<li>Wrong Binary Operator</li>
<li>Swapped Operand</li>
<li>Function-Docstring Mismatch</li>
<li>Exception Type</li>
</ol>
</li>
<li><p>一个定位+修复任务（Variable-Misuse Localization and Repair），也是本文唯一一个非分类任务</p>
</li>
</ol>
<p>就这几个下游任务而言，可以看到 CuBERT 主要还是在做代码理解领域的判别式任务，与后续出现的 CodeXGLUE Benchmark  比其在任务的数量和类型上都有局限性，也导致了这个 Benchmark 没有被广泛使用。而且，由于它仅采用了 Python 语言，和后面出现的各种 CodePTMs 比局限性也比较大，因此仅做简单的科普。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230530213325518.png" alt="image-20230530213325518"></p>
<h2 id="CodeBERT"><a href="#CodeBERT" class="headerlink" title="CodeBERT"></a><strong>CodeBERT</strong></h2><p>CodeBERT: A Pre-Trained Model for Programming and Natural Languages. Findings of EMNLP 2020.</p>
<p><em><a target="_blank" rel="noopener" href="https://aclanthology.org/2020.findings-emnlp.139.pdf">https://aclanthology.org/2020.findings-emnlp.139.pdf</a></em></p>
<p>相比前面提到同期工作的 CuBERT，CodeBERT 的影响力比它大很多。一方面是因为它是多语言（multi-programming-lingual）模型，纳入了 6 个编程语言，另一方面是它和 MSRA 自己的 CodeXGLUE Benchmark 配套后在各个下游任务上被广泛使用。</p>
<p>除了预训练阶段的任务有变化外，CodeBERT 的其他方面与自然语言中的 BERT 模型训练基本无异（其本质上的一个 RoBERTa），CodeBERT 使用了 bimodal data（即 PL-NL Pairs）进行了预训练，预训练数据的来源为 CodeSearchNet 数据集 [2]，其中有 Python, Java, JavaScript, PHP, Ruby 和 Go 这六种编程语言的 2.1M bimodal data 和  6.4M unimodal codes（也就是没有对应 comments 的纯代码），这些数据的来源都是 GitHub  中的开源仓库，并且后续的很多工作也在预训练阶段用了 CodeSearchNet 数据集。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640.png" alt="图片"></p>
<p>▲ 表中为CodeSearchNet的数据概览</p>
<p>CodeBERT 的输入形式为：</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-16854536202351.png" alt="图片"></p>
<p>第一段为自然语言文本，第二段为代码，训练的数据可分为两种，即 bimodal data，即 NL-PL Pairs 和 unimodal data，也就是纯代码。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-16854536202362.png" alt="图片"></p>
<p>▲ bimodal data</p>
<p>CodeBERT 在预训练阶段的任务有以下两个：</p>
<p><strong>Masked Language Modeling（MLM）</strong>，算是 Transformer 类模型的预训练中最老生常谈的任务了，作者将其应用于基于 bimodal data 的训练。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-16854536202363.png" alt="图片"></p>
<p>▲ Masked Language Modeling</p>
<p><strong>Replaced Token Detection（RTD）</strong>，迁移自 ELECTRA，既可以利用 bimodal data 进行训练，还可以进一步利用 unimodal data（比如是没有对应自然语言文本的 code），具体细节可以参考 ELECTRA 原文。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-16854536202364.png" alt="图片"></p>
<p>▲ Replaced Token Detection</p>
<p>实验部分做了 Natural Language Code Search，个人认为文中没有添加更多下游任务是受到 EMNLP 的篇幅限制，使用  CodeBERT 可以在多个下游任务，如 Clone detection（克隆检测）、Defect detection（缺陷检测）、Code  summarization 等上得到出色的结果，具体可参考 CodeXGLUE [3]，如下图所示：</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-16854536202365.jpeg" alt="图片"></p>
<p>▲ CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation</p>
<p>从 CodeBERT 开始，后续的 CodePTMs 就全部继承了对多个 PL 的支持，不过 CodeBERT 完全使用了建模自然语言的手段来为  Code（或是说 NL-PL  Pairs）做预训练，忽视了代码的一个很大的特性，那就是结构信息，具体而言就是在编译器进行语法分析阶段生成的抽象语法树（Abstract  Syntax Tree，本文称之为AST），紧跟着 CodeBERT 的 GraphCodeBERT 立刻填上了这个坑。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545385961418.png" alt="图片"></p>
<h2 id="GraphCodeBERT"><a href="#GraphCodeBERT" class="headerlink" title="GraphCodeBERT"></a><strong>GraphCodeBERT</strong></h2><p>GraphCodeBERT: Pre-training Code Representations with Data Flow. ICLR 2021.</p>
<p><em><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=jLoC4ez43PZ">https://openreview.net/forum?id=jLoC4ez43PZ</a></em></p>
<p>看名字就知道这是 CodeBERT 的后续工作，主要想法就是为 CodeBERT 添加代码的语法信息，使 CodePTM 可以显式学习代码的结构信息。</p>
<p>GraphCodeBERT 基于数据流学习代码的表征，如下图所示</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545385961519.png" alt="图片"></p>
<p>▲ GraphCodeBERT所使用的数据流</p>
<p>数据流的获得分为以下几个步骤：</p>
<ol>
<li>通过语法分析工具获得 AST，原文中使用的工具是 tree-sitter。</li>
<li>从 AST 中提出变量，构成一个由变量组成的序列。</li>
<li>从 AST 中抽取变量之间的依赖关系，文中称之为“value comes from”，构造数据流图。</li>
</ol>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545385961520.png" alt="图片"></p>
<p>▲ GraphCodeBERT的预训练</p>
<p>GraphCodeBERT 在模型预训练阶段额外提出了两个在当时较为新颖的训练任务</p>
<ol>
<li>Edge Prediction，即数据流图边预测，通过预测数据流图的边学习代码的结构信息。</li>
<li>Node Alignment，即变量对齐，具体而言是学习数据流图中的某个 node 来自输入代码中的哪个 code token。</li>
</ol>
<p>将它们和从 CodeBERT（或是 BERT or RoBERTa）继承下来的 MLM 任务一起优化。考虑到 AST 是一种图结构，为了让  Transformer 能适应其与一般序列结构的差异，作者修改了其注意力机制，主要是通过调整 Attention Mask 缩小感受野。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545385961521.png" alt="图片"></p>
<p>作者将其称为 Graph-Guided Masked Attention，其中 E 代表的是数据流图的边，E’ 代表的是数据流图的节点和代码的对应关系边。</p>
<p>就下游任务而言，GraphCodeBERT 文中主要完成了 Natural Language Code Search、Clone Detection、Code Translation 和 Code Refinement 这几个任务，它同样适用 CodeXGLUE Benchmark 中的其他任务，比如 Code  Summarization 等。</p>
<p>GraphCodeBERT 相较于前作 CodeBERT 解决了 CodePTM 只学习自然语义，而不学代码结构&#x2F;语法的问题。但细心的读者或许能发现，学习数据流相较于学习 AST 本身有相当的信息损失，这也为之后的 UniXcoder 挖了一个小坑。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545404615130.png" alt="图片"></p>
<h2 id="GPT-C"><a href="#GPT-C" class="headerlink" title="GPT-C"></a><strong>GPT-C</strong></h2><p>IntelliCode Compose: Code Generation Using Transformer. FSE&#x2F;ESEC 2020.</p>
<p><em><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.08025">https://arxiv.org/abs/2005.08025</a></em></p>
<p>GPT-C 是为了代码补全（Code Completion）这个任务而设计的，作者认为之前的的代码补全工作有两点不同。</p>
<ol>
<li>根据上文的 token 来预测下个 token，没有将代码的全文环境纳入考虑；</li>
<li>多语言效果不佳。</li>
</ol>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545404615131.png" alt="图片"></p>
<p>▲ Code Completion</p>
<p>作者提出的 GPT-C 是 GPT-2 模型的变体，在一个大规模、无监督、多语言的数据集上从零开始训练。基于 GPT-C，作者构建了一个代码补全  Framework，称之为 IntelliCode Compose，并对多种编程语言进行建模。作者将 Sequence decoding  的过程视为对树的搜索，搜到出现目标 token 为止。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545404615132.png" alt="图片"></p>
<p>▲ Sequence decoding</p>
<p>虽说是多语言，但是使用的是 Python, C#, JavaScript 和 TypeScript，和 CodeXGLUE 不同且少了两个语言。就多语言模型的训练而言，作者提出了四个训练的策略</p>
<ol>
<li>Language-agnostic baseline，即忽略掉编程语言的不同构建一个 baseline 多语言模型。</li>
<li>Language-type embedding，即加入一个向量来表示每种编程语言，和 token embedding 等相加。</li>
<li>Language-specific control codes，每个输入的训练样本前拼接一个”lang ∗ remaining token sequence”字符串，∗即为编程语言。</li>
<li>add a programming language classification task during model pretraining，即在预训练阶段加入一个分类编程语言的任务。</li>
</ol>
<p>随后在下游任务上验证了这几个方案的效果，证明了 Language-type embedding 和 control codes 方案在对各编程语言建模方面的有效性。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545404615133.png" alt="图片"></p>
<p>▲ multilingual modeling approaches based on GPT-C</p>
<p>在文末，作者考虑到了模型的推理开销问题，还上了一个知识蒸馏，并且还讨论了一下模型基于 K8S 和 VS Code 的部署应用等问题。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545437708542.png" alt="图片"></p>
<h2 id="Code-GPT"><a href="#Code-GPT" class="headerlink" title="Code-GPT"></a><strong>Code-GPT</strong></h2><p>Code-GPT 是在 CodeXGLUE 中被提出的，没有单独成文，不要和 GPT-C 搞混了。作者实现它的目的是为了 code completion 和  text-to-code generation 任务。它就是一个由 Code 训练，与 GPT-2 完全同架构的 12 层  Transformer Decoder 模型，不过 MSRA 的研究者实现了两个版本。</p>
<ol>
<li>Pretrained from scratch：随机初始化，从零训练；</li>
<li>CodeGPT-adapted：先使用一个 GPT-2 作为起始点，再持续使用 Code 进行训练，作者将这个方法称为“domain-adaptive”。</li>
</ol>
<p>更详细的内容可以参考 CodeXGLUE 原文的 4.2 节，作者在 Huggingface 提供了 CodeGPT-small-java [4] 和 CodeGPT-small-java-adapted [5] 这两个 checkpoints，正常地使用 transformers 库加载就能使用了。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545440175345.png" alt="图片"></p>
<h2 id="PLBART"><a href="#PLBART" class="headerlink" title="PLBART"></a><strong>PLBART</strong></h2><p>Unified Pre-training for Program Understanding and Generation. NAACL 2021.</p>
<p><em><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.06333">https://arxiv.org/abs/2103.06333</a></em></p>
<p>顾名思义，就是应用于编程语言的 BART，参考了文章：BART: Denoising Sequence-to-Sequence Pre-training for  Natural Language Generation, Translation, and Comprehension（ACL 2020）[6]。</p>
<p>先简单说下 BART，它吸收了 BERT 模型的双向编码器和 GPT 中的单向 left-to-right 解码器这二者的优点，因此比 BERT  更适合文本生成的场景（BERT 因 Pre-training 阶段和生成式下游任务差异比较大，因此被认为不适合做 NLG 相关任务），而它相比  GPT，也多了双向上下文语境信息（GPT 是单向建模）。除此之外，相比 BERT 的 Token Masking，BART 对 Encoder  端采用了更加复杂的Noise。</p>
<p>基于对 BART 的知识，理解 PLBART 并不算困难，其使用了和 BART-base 相同的架构，唯一结构上的不同的是它在 Encoder 和  Decoder 的顶部添加了一个额外的 LayerNorm。在 Noise 策略方面，PLBART 使用了 token masking,  token deletion 和 token infilling 这三种策略，与 BART 相比少了 Sentence Permutation 和 Document Rotation 这两个任务，这些任务的细节都可以参考 BART 原文。</p>
<p>在微调下游任务方面，作者将 PLBART 的下游任务分为两块。</p>
<p>首先是 Sequence Generation，又可细分为三个任务，可参考下图：</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545440175346.png" alt="图片"></p>
<p>▲ PLBART: Sequence Generation</p>
<p>其次是 Sequence Classification：将序列尾部的 special token 喂给线性分类器用于预测，与 BERT 等模型的分类区别不大。</p>
<p>实验与比较方面，作者先指定了 baseline 模型，并将其分成了两种：</p>
<ol>
<li>Training from Scratch，作者用下游任务的数据集从零开始训练了 LSTM + Attention 以及一个 Transformer。</li>
<li>Pre-trained Models，作者挑选了 RoBERTa、CodeBERT、GraphCodeBERT、GPT-2、CodeGPT（-adapted）。</li>
</ol>
<p>具体的实验部分做了 Code Summarization、Code Generation、Code Translation 这三个生成式任务，效果自然是好的，在  Classification 方面做了两个任务：clone detection 和 vulnerability detection，在后者上  PLBART 不如基于 AST 的模型。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545549204751.png" alt="图片"></p>
<h2 id="CodeT5"><a href="#CodeT5" class="headerlink" title="CodeT5"></a><strong>CodeT5</strong></h2><p>CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. EMNLP 2021.</p>
<p><em><a target="_blank" rel="noopener" href="https://aclanthology.org/2021.emnlp-main.685/">https://aclanthology.org/2021.emnlp-main.685/</a></em></p>
<p>文中对 CodeT5 的描述是：a unified pre-trained encoder-decoder Transformer model  that better leverages the code semantics conveyed from the  developer-assigned identifiers，即一个能更好地利用代码语法信息（形式是  identifier，即标识符）的统一预训练 Transformer 模型。在开始之前，和 PLBART 一样，先简单说下 Google T5  模型。T5 的名字来源是 Text-To-Text Transfer Transformer，顾名思义 T5 把所有的 NLP 问题统一归纳为了 Text2Text 任务，用来做 NMT、QA、文本摘要和文本分类任务。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545549204752.png" alt="图片"></p>
<p>▲ Text-To-Text Transfer Transformer</p>
<p>对比下 T5 原文，可以发现二者的核心思想还是非常类似的，作者将 CodeT5 归纳为 a pre-trained encoder-decoder  model that considers the token type information in code，细心的玩家可能发现了，前面提到的 CodeBERT 为首的 BERT 类模型和 CodeGPT 为首的 GPT 类模型，仅含有 Encoder 或  Decoder，而非完整利用一个 Transformer 架构来处理代码。因此 CodeT5 最大的卖点即第一个 unified  encoder-decoder CodePTM，可以理解为完全使用了 Transformer 的两个部分。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545549204753.png" alt="图片"></p>
<p>▲ CodeT5 Illustration</p>
<p>此外，除了使用 T5 的架构外，作者使用了以下两个方案来更好地利用代码结构特性：</p>
<ul>
<li>使用了代码的标识符信息，提出了 Identifier-aware Pre-training，是一个与 T5中Masked Span 类似的目标，简而言之就是随机 mask 掉任意长度的 Span，然后让 decoder 去预测。</li>
<li>利用了代码地 Comments（自然语言注释）信息，作者称之为 Bimodel Dual Generation，让自然语言和源代码的表征可以对齐，帮助缓和 Pre-training 和 Fine-Tuning 阶段的差距。</li>
</ul>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545549204754.png" alt="图片"></p>
<p>▲ Identifier-aware Pre-training</p>
<p>文章发表时在 CodeXGLUE Benchmark 的若干任务上取得了 SOTA 效果。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/image-20230530222936538.png" alt="image-20230530222936538"></p>
<h2 id="UniXcoder"><a href="#UniXcoder" class="headerlink" title="UniXcoder"></a><strong>UniXcoder</strong></h2><p>UniXcoder: Unified Cross-Modal Pre-training for Code Representation. ACL 2022.</p>
<p><em><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.03850.pdf">https://arxiv.org/pdf/2203.03850.pdf</a></em></p>
<p>这是当今（2022.7）的 SOTA 模型，参考了 NIPS 2019: Unified Language Model Pre-training for Natural Language Understanding and Generation [7]。</p>
<p>文章选择了五个任务，分为以下三类：</p>
<ol>
<li>代码理解任务：Clone Detection 和 Code Search</li>
<li>代码生成任务：Code Summary 和 Code Generation</li>
<li>自回归任务：Code Completion</li>
</ol>
<p>本文很重要的一个卖点就是更全面地利用了 AST 提供的代码结构信息。文章开头讲过，AST 一般会被表示为一个 Tree 结构，不能直接作为 Transformer  类模型的输入，回忆一下前面提到的 GraphCodeBERT，作者在以损失相当一部分信息的情况下让模型学习 AST  的数据流。为了能更加有效地利用 AST，因此 UniXcoder 地作者构建了一个 one-to-one 的 mapping  function，将 AST 转为一个序列结构（flattened token sequence），然后和Code  Comments一同编码，对这个 mapping function 的有效性的证明在文章的附录中。</p>
<p>模型结构方面，UniXcoder 的一个卖点就是一个统一的，可以同时兼容 Encoder-Only，Decoder-Only 和 Encoder-Decoder 三种模式的  CodePTM，相当于给模型添加了“开关”，来决定采用什么模式处理任务，用白话讲，就是通过使用三种不同类型的自注意力 Mask  策略来控制模型的行为。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545694764864.png" alt="图片"></p>
<p>▲ UniXcoder的架构</p>
<p>既然同时能拥有三种模式，那么自然会有更多预训练任务，如下所示：</p>
<ol>
<li>Masked Language Modeling（MLM），算是基本操作了。</li>
<li>Unidirectional Language Modeling（ULM），用于训练 decoder-only 模式，帮助完成自回归任务，对应的是右上三角 masking。</li>
<li>Denoising Objective DeNoiSing（DNS），可参考 BART 和 T5，用于训练 encoder-decoder 模式，帮助完成生成任务，参考架构图中的 encoder-decoder 部分。</li>
</ol>
<p>除了上面这些任务以外，作者还提出了 Code Fragment Representation Learning。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545694764965.png" alt="图片"></p>
<p>▲ Code Fragment Representation Learning</p>
<p>其中包含了 Multi-modal Contrastive Learning（MCL）和 Cross-Modal  Generation（CMG）这两个任务。前者采用了一个对比学习损失，后者是使用了一个统一的自然语言描述（comment），文中使用了  fulcrum，即支点这个词，让模型学习到的代码表征在不同语言之间的对齐。</p>
<p>还需注意的一点就是，UniXcoder 在预训练和微调这两个阶段中的输入形式有所不同，由于引入了 Flattened AST，AST 展开后的序列中被引入了大量额外的  tokens（70% longer）会导致额外的开销。因此，在微调阶段 UniXcoder 仅使用 AST 的叶子节点，为了缓解这个  gap，在预训练阶段作者设置了 0.5 的概率随机丢弃输入序列中的非叶子节点。</p>
<p>除了 Clone Detection、Code Search、Code Summarization 和 Code Completion  等任务上表现较好外，UniXcoder 还提供了一个新任务：zero-shot code-to-code search，即在 zero-shot 的情境下，通过一个源代码的 query 在 candidates 集合中寻找语义相同的源代码，该任务使用的数据集是 CodeNet [8]，用来衡量训练所得的 code fragment embeddings 的效果。</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545694764966.png" alt="图片"></p>
<p>▲ zero-shot code-to-code search</p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545709265475.png" alt="图片"></p>
<h2 id="相关代码整理"><a href="#相关代码整理" class="headerlink" title="相关代码整理"></a><strong>相关代码整理</strong></h2><p><strong>CuBERT：</strong></p>
<p><em><a target="_blank" rel="noopener" href="https://github.com/google-research/google-research/tree/master/cubert">https://github.com/google-research/google-research/tree/master/cubert</a></em></p>
<p><strong>CodeBERT、GraphCodeBERT 和 UniXCoder</strong></p>
<p>MSRA 提供了 CodeBERT、GraphCodeBERT 和 UniXCoder 在下游任务微调时可用的代码，在仓库：</p>
<p><em><a target="_blank" rel="noopener" href="https://github.com/microsoft/CodeBERT">https://github.com/microsoft/CodeBERT</a></em></p>
<p>但没有提供预训练阶段的实现（CodeBERT 和 UniXCoder 在预训练阶段都使用了 16  张 32GB  NVIDIA Tesla V100 实现），使用时通过 transformers 加载 checkpoints 就可使用。</p>
<p>此外，huggingface 还提供了一个经济适用版的 CodeBERT 模型：</p>
<p><em><a target="_blank" rel="noopener" href="https://huggingface.co/huggingface/CodeBERTa-small-v1">https://huggingface.co/huggingface/CodeBERTa-small-v1</a></em></p>
<p><strong>CodeGPT</strong></p>
<p>与上述三个 MSRA 提供的模型一样，CodeGPT 仍然是提供了可通过 transformers 加载 checkpoints，即：</p>
<p><em><a target="_blank" rel="noopener" href="https://huggingface.co/microsoft/CodeGPT-small-java-adaptedGPT2">https://huggingface.co/microsoft/CodeGPT-small-java-adaptedGPT2</a></em></p>
<p><strong>CodeT5：</strong></p>
<p><em><a target="_blank" rel="noopener" href="https://github.com/salesforce/codet5">https://github.com/salesforce/codet5</a></em></p>
<p><strong>PLBART：</strong></p>
<p><em><a target="_blank" rel="noopener" href="https://github.com/wasiahmad/PLBART">https://github.com/wasiahmad/PLBART</a></em></p>
<p><img src="/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/640-168545711337578.png" alt="图片"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>上述对 CodePTMs  相关的内容大致上是六月中下旬赶一个文章时调研文献后总结的笔记，然后补充了点链接和细节，实验部分写的比较简略，是因为如果每个模型的实验都要全讲的话篇幅就太长了，而且这些任务都大差不差，每个模型都讲一遍冗余会比较多，之后可能会在其他文章补充。</p>
<p>此外，有一些影响力比较小或者 Task-specific 的工作可能没完全覆盖到。总而言之，不论是 CodeBERT 开的新坑还是今天的 SOTA 模型  UniXcoder，MSRA 在这个领域还是完全 dominant 的存在。对于 CodeBERT 和 GraphCodeBERT  为首的大模型，复现预训练阶段的成本很高，不适合平民玩家，而且今年五月的 IJCAI 22 Survey Track 连 CodePTMs 的  Survey 工作都已经出了（Deep Learning Meets Software Engineering: A Survey on  Pre-Trained Models of Source Code [9]），可能短时间内出革命性的新模型的可能性不大，而且 Code 领域使用的这些方法终究还是跟着 NLP 走的，需要 NLP 提出新技术后 Code 领域才有跟进的可能。</p>
<p>个人感觉接下来在这个相对较小但很卷的领域的研究热点可能会慢慢向可解释性和模型分析（Analysis of Models &amp; Interpretability）方面转移，最近还研读了一些 22 年新出的 Probing CodePTMs 的文章，之后再补充。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Lin Li</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/">http://example.com/2023/06/03/%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">LinLi's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/06/19/Restoring-the-Executability-of-Jupyter-Notebooks-by-Automatic-Upgrade-of-Deprecated-APIs/" title="Restoring the Executability of Jupyter Notebooks by Automatic Upgrade of Deprecated APIs"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Restoring the Executability of Jupyter Notebooks by Automatic Upgrade of Deprecated APIs</div></div></a></div><div class="next-post pull-right"><a href="/2023/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F-%E4%BB%8EWord-Embedding%E5%88%B0BERT/" title="预训练语言模型的前世今生 - 从Word Embedding到BERT"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">预训练语言模型的前世今生 - 从Word Embedding到BERT</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Lin Li</div><div class="author-info__description">今日事，今日毕</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">63</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E8%A1%A8%E5%BE%81%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%EF%BC%9A%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E4%BB%A3%E7%A0%81"><span class="toc-number">1.</span> <span class="toc-text">代码表征预训练语言模型学习指南：原理、分析和代码</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CuBERT"><span class="toc-number">1.1.</span> <span class="toc-text">CuBERT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CodeBERT"><span class="toc-number">1.2.</span> <span class="toc-text">CodeBERT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GraphCodeBERT"><span class="toc-number">1.3.</span> <span class="toc-text">GraphCodeBERT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPT-C"><span class="toc-number">1.4.</span> <span class="toc-text">GPT-C</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Code-GPT"><span class="toc-number">1.5.</span> <span class="toc-text">Code-GPT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PLBART"><span class="toc-number">1.6.</span> <span class="toc-text">PLBART</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CodeT5"><span class="toc-number">1.7.</span> <span class="toc-text">CodeT5</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#UniXcoder"><span class="toc-number">1.8.</span> <span class="toc-text">UniXcoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E6%95%B4%E7%90%86"><span class="toc-number">1.9.</span> <span class="toc-text">相关代码整理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.10.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/09/History-Driven-Build-Failure-Fixing-How-Far-Are-We/" title="History-Driven Build Failure Fixing: How Far Are We?">History-Driven Build Failure Fixing: How Far Are We?</a><time datetime="2023-07-09T07:58:11.000Z" title="发表于 2023-07-09 15:58:11">2023-07-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/09/HireBuild-An-Automatic-Approach-to-History-Driven-Repair-of-Build-Scripts/" title="HireBuild: An Automatic Approach to History-Driven Repair of Build Scripts">HireBuild: An Automatic Approach to History-Driven Repair of Build Scripts</a><time datetime="2023-07-09T07:57:05.000Z" title="发表于 2023-07-09 15:57:05">2023-07-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/07/Automated-API-Usage-Update-for-Android-Apps/" title="Automated API-Usage Update for Android Apps">Automated API-Usage Update for Android Apps</a><time datetime="2023-07-07T08:20:20.000Z" title="发表于 2023-07-07 16:20:20">2023-07-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/07/Automated-Deprecated-API-Usage-Update-for-Android-Apps-How-Far-Are-We/" title="Automated Deprecated-API Usage Update for Android Apps How Far Are We">Automated Deprecated-API Usage Update for Android Apps How Far Are We</a><time datetime="2023-07-07T08:19:22.000Z" title="发表于 2023-07-07 16:19:22">2023-07-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/06/28/AexPy-Detecting-API-Breaking-Changes-in-Python-Packages/" title="AexPy Detecting API Breaking Changes in Python Packages">AexPy Detecting API Breaking Changes in Python Packages</a><time datetime="2023-06-28T12:42:06.000Z" title="发表于 2023-06-28 20:42:06">2023-06-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Lin Li</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>